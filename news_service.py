"""
Ø®Ø¯Ù…Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± - Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©
News Service - Fetches news from multiple sources
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

class NewsAggregator:
    """Ù…Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©"""

    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ar,en;q=0.9',
    }

    # Cache Ù„Ù„Ø£Ø®Ø¨Ø§Ø±
    _news_cache = {}
    _cache_time = None
    CACHE_DURATION = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚

    @classmethod
    def get_all_news(cls, limit: int = 50) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
        if cls._cache_time and (datetime.now() - cls._cache_time).seconds < cls.CACHE_DURATION:
            if 'all' in cls._news_cache:
                return cls._news_cache['all'][:limit]

        all_news = []

        # Ø¬Ù„Ø¨ Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(cls.get_argaam_news): 'argaam',
                executor.submit(cls.get_aleqt_news): 'aleqt',
                executor.submit(cls.get_maaal_news): 'maaal',
            }

            for future in as_completed(futures):
                source = futures[future]
                try:
                    news = future.result()
                    all_news.extend(news)
                except Exception as e:
                    print(f"Error fetching {source}: {e}")

        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
        all_news.sort(key=lambda x: x.get('date', ''), reverse=True)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
        seen = set()
        unique_news = []
        for news in all_news:
            title = news.get('title', '')
            if title and title not in seen:
                seen.add(title)
                unique_news.append(news)

        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
        cls._news_cache['all'] = unique_news
        cls._cache_time = datetime.now()

        return unique_news[:limit]

    @classmethod
    def get_argaam_news(cls, limit: int = 20) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø£Ø±Ù‚Ø§Ù…"""
        news = []
        try:
            resp = requests.get(
                'https://www.argaam.com/ar',
                headers=cls.HEADERS,
                timeout=15
            )

            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')

                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
                links = soup.find_all('a', href=lambda x: x and '/ar/article/articledetail' in str(x))

                seen_ids = set()
                for link in links:
                    href = link.get('href', '')

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID Ø§Ù„Ù…Ù‚Ø§Ù„
                    article_id = ''
                    if '/id/' in href:
                        article_id = href.split('/id/')[-1].split('/')[0].split('?')[0]

                    if not article_id or article_id in seen_ids:
                        continue
                    seen_ids.add(article_id)

                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                    title = link.get_text(strip=True)

                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                    if not title or len(title) < 10:
                        continue

                    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
                    title = re.sub(r'(Ø®Ø§Øµ|Ø­ØµØ±ÙŠ|Ù…Ø®ØªØ§Ø±Ø§Øª Ø£Ø±Ù‚Ø§Ù…|ØªÙ‚Ø§Ø±ÙŠØ± Ø£Ø±Ù‚Ø§Ù…)', '', title).strip()

                    if len(title) > 10:
                        news.append({
                            'id': article_id,
                            'title': title[:150],
                            'url': f"https://www.argaam.com{href}" if href.startswith('/') else href,
                            'source': 'Ø£Ø±Ù‚Ø§Ù…',
                            'source_icon': 'ğŸ“Š',
                            'date': datetime.now().strftime('%Y-%m-%d'),
                            'category': 'Ø£Ø³ÙˆØ§Ù‚'
                        })

                    if len(news) >= limit:
                        break

        except Exception as e:
            print(f"Argaam error: {e}")

        return news

    @classmethod
    def get_argaam_article_content(cls, article_id: str) -> Optional[Dict]:
        """Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ù…Ù‚Ø§Ù„ Ù…Ù† Ø£Ø±Ù‚Ø§Ù…"""
        try:
            url = f"https://www.argaam.com/ar/article/articledetail/id/{article_id}"
            resp = requests.get(url, headers=cls.HEADERS, timeout=15)

            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')

                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                paragraphs = soup.find_all('p')
                content = []

                for p in paragraphs:
                    text = p.get_text(strip=True)
                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹
                    if len(text) > 30 and 'Ø£Ø±Ù‚Ø§Ù…' not in text[:20] and 'ØªØ³Ø¬ÙŠÙ„' not in text[:20]:
                        content.append(text)

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                title_tag = soup.find('h1') or soup.find('h2')
                title = title_tag.get_text(strip=True) if title_tag else ''

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®
                date_text = ''
                date_tag = soup.find(['time', 'span'], class_=lambda x: x and 'date' in str(x).lower())
                if date_tag:
                    date_text = date_tag.get_text(strip=True)

                return {
                    'id': article_id,
                    'title': title,
                    'content': '\n\n'.join(content[:15]),  # Ø£ÙˆÙ„ 15 ÙÙ‚Ø±Ø©
                    'url': url,
                    'date': date_text,
                    'source': 'Ø£Ø±Ù‚Ø§Ù…'
                }

        except Exception as e:
            print(f"Error fetching article {article_id}: {e}")

        return None

    @classmethod
    def get_aleqt_news(cls, limit: int = 15) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©"""
        news = []
        try:
            resp = requests.get(
                'https://www.aleqt.com/',
                headers=cls.HEADERS,
                timeout=15
            )

            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')

                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
                links = soup.find_all('a', href=True)

                seen_titles = set()
                for link in links:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)

                    # ÙÙ„ØªØ±Ø©
                    if not title or len(title) < 20 or len(title) > 200:
                        continue

                    if title in seen_titles:
                        continue

                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ø©
                    if any(x in title for x in ['ØªØ³Ø¬ÙŠÙ„', 'Ø§Ù„Ø¯Ø®ÙˆÙ„', 'Ø§Ø´ØªØ±Ùƒ', 'Ø§Ù„Ø¨Ø­Ø«', 'Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©']):
                        continue

                    seen_titles.add(title)

                    full_url = href if href.startswith('http') else f"https://www.aleqt.com{href}"

                    news.append({
                        'title': title,
                        'url': full_url,
                        'source': 'Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©',
                        'source_icon': 'ğŸ“°',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'category': 'Ø§Ù‚ØªØµØ§Ø¯'
                    })

                    if len(news) >= limit:
                        break

        except Exception as e:
            print(f"Aleqt error: {e}")

        return news

    @classmethod
    def get_maaal_news(cls, limit: int = 15) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…Ø§Ù„"""
        news = []
        try:
            resp = requests.get(
                'https://maaal.com/',
                headers=cls.HEADERS,
                timeout=15
            )

            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')

                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
                articles = soup.find_all(['article', 'div'], class_=lambda x: x and any(k in str(x).lower() for k in ['post', 'article', 'entry']))

                for article in articles[:limit]:
                    link = article.find('a', href=True)
                    if not link:
                        continue

                    href = link.get('href', '')

                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                    title_tag = article.find(['h2', 'h3', 'h4', 'a'])
                    title = title_tag.get_text(strip=True) if title_tag else ''

                    if not title or len(title) < 15:
                        continue

                    news.append({
                        'title': title[:150],
                        'url': href if href.startswith('http') else f"https://maaal.com{href}",
                        'source': 'Ù…Ø§Ù„',
                        'source_icon': 'ğŸ’°',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'category': 'Ø£Ø¹Ù…Ø§Ù„'
                    })

        except Exception as e:
            print(f"Maaal error: {e}")

        return news

    @classmethod
    def search_news(cls, query: str, limit: int = 20) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
        all_news = cls.get_all_news(100)

        query_lower = query.lower()
        results = []

        for news in all_news:
            title = news.get('title', '').lower()
            if query_lower in title:
                results.append(news)
                if len(results) >= limit:
                    break

        return results

    @classmethod
    def get_news_by_source(cls, source: str, limit: int = 20) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ¯Ø± Ù…Ø­Ø¯Ø¯"""
        source_lower = source.lower()

        if 'Ø£Ø±Ù‚Ø§Ù…' in source or 'argaam' in source_lower:
            return cls.get_argaam_news(limit)
        elif 'Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©' in source or 'aleqt' in source_lower:
            return cls.get_aleqt_news(limit)
        elif 'Ù…Ø§Ù„' in source or 'maaal' in source_lower:
            return cls.get_maaal_news(limit)
        else:
            return cls.get_all_news(limit)

    @classmethod
    def clear_cache(cls):
        """Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´"""
        cls._news_cache = {}
        cls._cache_time = None


# Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…
class NewsService:
    """ÙˆØ§Ø¬Ù‡Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…"""

    @staticmethod
    def get_stock_news(symbol: str) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø³Ù‡Ù…"""
        code = symbol.strip().replace(".SR", "")

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø®Ø¨Ø§Ø± Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø³Ù‡Ù…
        all_news = NewsAggregator.get_all_news(50)

        # Ù„Ù„Ø£Ø³Ù Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø±Ø¨Ø· Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø§Ù„Ø£Ø³Ù‡Ù… Ø¨Ø¯Ù‚Ø© Ø¨Ø¯ÙˆÙ† API
        # Ù†Ø±Ø¬Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø©
        return all_news[:10]

    @staticmethod
    def get_portfolio_news(symbols: List[str]) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø­ÙØ¸Ø©"""
        return NewsAggregator.get_all_news(20)

    @staticmethod
    def get_saudi_market_news() -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ"""
        return NewsAggregator.get_all_news(30)
